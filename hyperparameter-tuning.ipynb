{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68699,"databundleVersionId":7659021,"sourceType":"competition"},{"sourceId":7908592,"sourceType":"datasetVersion","datasetId":4645838}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier \nimport optuna\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport pandas as pd\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:35:54.332552Z","iopub.execute_input":"2024-03-21T20:35:54.333685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s4e3/train.csv\")\ntrain_original = pd.read_csv(\"/kaggle/input/faults/faults.csv\")\n\ntrain.drop(columns=['id'], axis=1, inplace=True)\ntarget_cols = ['Pastry','Z_Scratch','K_Scatch','Stains','Dirtiness','Bumps','Other_Faults']\ntrain = train[train[target_cols].sum(axis=1) == 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.concat([train_original, train], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_generator(data):\n\n    \"A function to generate additional features\"\n\n    epsilon = 1e-6  # A small constant to avoid division by zero or taking the logarithm of zero\n    # Location Features\n    data['X_Distance'] = data['X_Maximum'] - data['X_Minimum']\n    data['Y_Distance'] = data['Y_Maximum'] - data['Y_Minimum']\n\n    # Density Feature\n    data['Density'] = data['Pixels_Areas'] / (data['X_Perimeter'] + data['Y_Perimeter'])\n\n    # Relative Perimeter Feature\n    data['Relative_Perimeter'] = data['X_Perimeter'] / (data['X_Perimeter'] + data['Y_Perimeter'] + epsilon)\n\n    # Circularity Feature\n    data['Circularity'] = data['Pixels_Areas'] / (data['X_Perimeter'] ** 2)\n\n    # Symmetry Index Feature\n    data['Symmetry_Index'] = np.abs(data['X_Distance'] - data['Y_Distance']) / (data['X_Distance'] + data['Y_Distance'] + epsilon)\n    \n    # Color Contrast Feature\n    data['Color_Contrast'] = data['Maximum_of_Luminosity'] - data['Minimum_of_Luminosity']\n\n    # Combined Geometric Index Feature\n    data['Combined_Geometric_Index'] = data['Edges_Index'] * data['Square_Index']\n\n    # Interaction Term Feature\n    data['X_Distance*Pixels_Areas'] = data['X_Distance'] * data['Pixels_Areas']\n    \n    # Additional Features\n    data['sin_orientation'] = np.sin(data['Orientation_Index'])\n    data['Edges_Index2'] = np.exp(data['Edges_Index'] + epsilon)\n    data['X_Maximum2'] = np.sin(data['X_Maximum'])\n    data['Y_Minimum2'] = np.sin(data['Y_Minimum'])\n    data['Aspect_Ratio_Pixels'] = np.where(data['Y_Perimeter'] == 0, 0, data['X_Perimeter'] / data['Y_Perimeter'])\n    data['Aspect_Ratio'] = np.where(data['Y_Distance'] == 0, 0, data['X_Distance'] / data['Y_Distance'])\n\n    # Average Luminosity Feature\n    data['Average_Luminosity'] = (data['Sum_of_Luminosity'] + data['Minimum_of_Luminosity']) / 2\n    \n    # Normalized Steel Thickness Feature\n    data['Normalized_Steel_Thickness'] = (data['Steel_Plate_Thickness'] - data['Steel_Plate_Thickness'].min()) / (data['Steel_Plate_Thickness'].max() - data['Steel_Plate_Thickness'].min())\n\n    # Logarithmic Features\n    data['Log_Perimeter'] = np.log(data['X_Perimeter'] + data['Y_Perimeter'] + epsilon)\n    data['Log_Luminosity'] = np.log(data['Sum_of_Luminosity'] + epsilon)\n    data['Log_Aspect_Ratio'] = np.log(data['Aspect_Ratio'] ** 2 + epsilon)\n\n    # Statistical Features\n    data['Combined_Index'] = data['Orientation_Index'] * data['Luminosity_Index']\n    data['Sigmoid_Areas'] = 1 / (1 + np.exp(-data['LogOfAreas'] + epsilon))\n\n    return data\n\ntest = feature_generator(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the scaler\nscaler = RobustScaler()\n\n# Identify discrete features\ndiscrete_features = ['TypeOfSteel_A300', 'TypeOfSteel_A400']\n\n# Identify continuous features\ncontinuous_features = [col for col in test.columns if col not in discrete_features + target_cols]\n\n# Fit and transform the scaler on training data\ntest[continuous_features] = scaler.fit_transform(test[continuous_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = test.drop(columns=target_cols, axis=1)\ny1 = test['Pastry']\ny2 = test['Z_Scratch']\ny3 = test['K_Scatch']\ny4 = test['Stains']\ny5 = test['Dirtiness']\ny6 = test['Bumps']\ny7 = test['Other_Faults']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_Pastry, X_test_pastry, y_train_Pastry, y_test_pastry = train_test_split(X, y1, test_size=0.3, random_state24)\nX_train_Z_Scratch, X_test_Z_Scratch, y_train_Z_Scratch, y_test_Z_Scratch = train_test_split(X, y2, test_size=0.3, random_state24)\nX_train_K_Scatch, X_test_K_Scatch, y_train_K_Scatch, y_test_K_Scatch = train_test_split(X, y3, test_size=0.3, random_state24)\nX_train_Stains, X_test_Stains, y_train_Stains, y_test_Stains = train_test_split(X, y4, test_size=0.3, random_state24)\nX_train_Dirtiness, X_test_Dirtiness, y_train_Dirtiness, y_test_Dirtiness = train_test_split(X, y5, test_size=0.3, random_state24)\nX_train_Bumps, X_test_Bumps, y_train_Bumps, y_test_Bumps = train_test_split(X, y6, test_size=0.3, random_state24)\nX_train_Other_Faults, X_test_Other_Faults, y_train_Other_Faults, y_test_Other_Faults = train_test_split(X, y7, test_size=0.3, random_state24)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming target_splits is the dictionary containing splits for each target\ntarget_splits = {\n    'Pastry': (X_train_Pastry, X_test_pastry, y_train_Pastry, y_test_pastry),\n    'Z_Scratch': (X_train_Z_Scratch, X_test_Z_Scratch, y_train_Z_Scratch, y_test_Z_Scratch),\n    'K_Scatch': (X_train_K_Scatch, X_test_K_Scatch, y_train_K_Scatch, y_test_K_Scatch),\n    'Stains': (X_train_Stains, X_test_Stains, y_train_Stains, y_test_Stains),\n    'Dirtiness': (X_train_Dirtiness, X_test_Dirtiness, y_train_Dirtiness, y_test_Dirtiness),\n    'Bumps': (X_train_Bumps, X_test_Bumps, y_train_Bumps, y_test_Bumps),\n    'Other_Faults': (X_train_Other_Faults, X_test_Other_Faults, y_train_Other_Faults, y_test_Other_Faults)\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna for xgb","metadata":{}},{"cell_type":"code","source":"# Define the objective function for XGBoost\ndef objective_xgb(trial, X_train, X_test, y_train, y_test):\n    params = {\n        \"objective\": \"binary:logistic\",\n        \"n_estimators\": 1000,\n        \"verbosity\": 0,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n        \"eval_metric\": 'auc',\n    }\n\n    model_xgb = XGBClassifier(**params)\n    model_xgb.fit(X_train, y_train, verbose=False)\n    predictions_xgb = model_xgb.predict(X_test)\n    accuracy_xgb = roc_auc_score(y_test, predictions_xgb)\n    return accuracy_xgb\n\n# Dictionary to store best models for each target\nbest_models = {}\n\n# Loop over each target\nfor target, (X_train, X_test, y_train, y_test) in target_splits.items():\n    print(f\"Tuning XGBoost for target: {target}\")\n    study_xgb = optuna.create_study(direction='maximize')\n    study_xgb.optimize(lambda trial: objective_xgb(trial, X_train, X_test, y_train, y_test), n_trials=30)\n    print('Best hyperparameters:', study_xgb.best_params)\n    print('Best accuracy:', study_xgb.best_value)\n\n    # Define the best hyperparameters obtained from Optuna\n    best_params_xgb = study_xgb.best_params\n\n    # Instantiate XGBClassifier with the best hyperparameters\n    best_xgb_model = xgb.XGBClassifier(**best_params_xgb, silent=True)\n\n    # Fit the model on the training data\n    best_xgb_model.fit(X_train, y_train)\n\n    # Save the model to a file\n    model_filename = f\"best_model_{target}.pkl\"\n    with open(model_filename, 'wb') as f:\n        pickle.dump(best_xgb_model, f)\n\n    # Store the model in the dictionary\n    best_models[target] = best_xgb_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna for lightgbm","metadata":{}},{"cell_type":"code","source":"def objective_lgbm(trial):\n    \"\"\"\n    Objective function to be minimized.\n    \"\"\"\n    lgbm_param = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"num_class\": 3,\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    \n    model_lgbm = LGBMClassifier(**params)\n    model_lgbm.fit(X_train, y_train, verbose=False)\n    predictions_lgbm = model_lgbm.predict(X_test)\n    accuracy_lgbm = roc_auc_score(y_test, predictions_lgbm)\n    return accuracy_lgbm\n\n\n# Dictionary to store best models for each target\nbest_models_lgbm = {}\n\n# Loop over each target\nfor target, (X_train, X_test, y_train, y_test) in target_splits.items():\n    print(f\"Tuning LGBM for target: {target}\")\n    study_lgbm = optuna.create_study(direction='maximize')\n    study_lgbm.optimize(lambda trial: objective_lgbm(trial, X_train, X_test, y_train, y_test), n_trials=30)\n    print('Best hyperparameters:', study_lgbm.best_params)\n    print('Best accuracy:', study_lgbm.best_value)\n\n    # Define the best hyperparameters obtained from Optuna\n    best_params_lgbm = study_lgbm.best_params\n\n    # Instantiate XGBClassifier with the best hyperparameters\n    best_lgbm_model = LGBMClassifier(**best_params_lgbm, silent=True)\n\n    # Fit the model on the training data\n    best_lgbm_model.fit(X_train, y_train)\n\n    # Save the model to a file\n    model_filename = f\"best_lightgbm_model_{target}.pkl\"\n    with open(model_filename, 'wb') as f:\n        pickle.dump(best_lgbm_model, f)\n\n    # Store the model in the dictionary\n    best_models_lgbm[target] = best_lgbm_model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna for catboost","metadata":{}},{"cell_type":"code","source":"# Define the objective function\ndef objective_cat(trial):\n    # Define the search space for CatBoost hyperparameters\n    cat_params = {\n        'iterations': trial.suggest_int('iterations', 100, 1000),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n        'depth': trial.suggest_int('depth', 1, 10),\n        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100)\n    }\n    model_cat = CatBoostClassifier(**params)\n    model_cat.fit(X_train, y_train, verbose=False)\n    predictions_cat = model_cat.predict(X_test)\n    accuracy_cat = roc_auc_score(y_test, predictions_cat)\n    return accuracy_cat\n\n# Dictionary to store best models for each target\nbest_models_cat = {}\n\n# Loop over each target\nfor target, (X_train, X_test, y_train, y_test) in target_splits.items():\n    print(f\"Tuning catBoost for target: {target}\")\n    study_cat = optuna.create_study(direction='maximize')\n    study_cat.optimize(lambda trial: objective_cat(trial, X_train, X_test, y_train, y_test), n_trials=30)\n    print('Best hyperparameters:', study_cat.best_params)\n    print('Best accuracy:', study_cat.best_value)\n\n    # Define the best hyperparameters obtained from Optuna\n    best_params_cat = study_cat.best_params\n\n    # Instantiate XGBClassifier with the best hyperparameters\n    best_cat_model = CatBoostClassifier(**best_params_cat, silent=True)\n\n    # Fit the model on the training data\n    best_cat_model.fit(X_train, y_train)\n\n    # Save the model to a file\n    model_filename = f\"best_cat_model_{target}.pkl\"\n    with open(model_filename, 'wb') as f:\n        pickle.dump(best_cat_model, f)\n\n    # Store the model in the dictionary\n    best_models_cat[target] = best_cat_model","metadata":{},"execution_count":null,"outputs":[]}]}